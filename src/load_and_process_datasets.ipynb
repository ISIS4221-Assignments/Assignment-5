{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d514a6c",
   "metadata": {},
   "source": [
    "# 1. Section Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import tarfile\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6c2106",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_DIR = \"../data\"\n",
    "\n",
    "datasets = {\n",
    "    \"20_news_dataset.tar.gz\": \"20_news\",\n",
    "    \"multi_domain_sentiment_dataset.tar.gz\": \"multi_domain_sentiment\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd7222",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(BASE_DATA_DIR, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d2ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gz_file, folder_name in datasets.items():\n",
    "\tgz_path = os.path.join(BASE_DATA_DIR, gz_file)\n",
    "\ttarget_dir = os.path.join(BASE_DATA_DIR, folder_name)\n",
    "\n",
    "\tif os.path.exists(target_dir) and len(os.listdir(target_dir)) > 0:\n",
    "\t\tprint(f\"Folder {folder_name} already extracted\")\n",
    "\t\tcontinue\n",
    "\n",
    "\tif os.path.exists(gz_path):\n",
    "\t\tprint(f\"Extracting {gz_file} into '{folder_name}'...\")\n",
    "\t\tos.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "\t\twith tempfile.TemporaryDirectory() as tmp_dir:\n",
    "\t\t\twith tarfile.open(gz_path, \"r:gz\") as tar:\n",
    "\t\t\t\ttar.extractall(path = tmp_dir)\n",
    "\n",
    "\t\t\tfor item in os.listdir(tmp_dir):\n",
    "\t\t\t\tsrc_path = os.path.join(tmp_dir, item)\n",
    "\t\t\t\tif os.path.isdir(src_path):\n",
    "\t\t\t\t\tfor sub_item in os.listdir(src_path):\n",
    "\t\t\t\t\t\tshutil.move(os.path.join(src_path, sub_item), target_dir)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tshutil.move(src_path, target_dir)\n",
    "\telse:\n",
    "\t\tprint(f\"File not found: {gz_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93930e66",
   "metadata": {},
   "source": [
    "# 2. Section Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e36ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_DIR = os.path.join(BASE_DATA_DIR, \"20_news\")\n",
    "MULTIDOMAIN_DIR = os.path.join(BASE_DATA_DIR, \"multi_domain_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_20_news(base_dir):\n",
    "\tdata = []\n",
    "\n",
    "\tfor category in os.listdir(base_dir):\n",
    "\t\tcategory_path = os.path.join(base_dir, category)\n",
    "\t\tif not os.path.isdir(category_path):\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tfor filename in os.listdir(category_path):\n",
    "\t\t\tfile_path = os.path.join(category_path, filename)\n",
    "\t\t\ttry:\n",
    "\t\t\t\twith open(file_path, \"rb\") as f:\n",
    "\t\t\t\t\traw = f.read()\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\ttext = raw.decode(\"utf-8\").strip()\n",
    "\t\t\t\texcept UnicodeDecodeError:\n",
    "\t\t\t\t\ttext = raw.decode(\"latin-1\").strip()\n",
    "\n",
    "\t\t\t\tdata.append({\n",
    "\t\t\t\t\t\"label\": category, \n",
    "\t\t\t\t\t\"document\": text\n",
    "\t\t\t\t})\n",
    "\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f\"Error reading {file_path}: {e}\")\n",
    "\t\n",
    "\treturn pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf056b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multidomain(base_dir):\n",
    "\tdata = []\n",
    "\n",
    "\tfor domain in os.listdir(base_dir):\n",
    "\t\tdomain_path = os.path.join(base_dir, domain)\n",
    "\t\tif not os.path.isdir(domain_path):\n",
    "\t\t\tcontinue\n",
    "\t\n",
    "\t\tfor filename in os.listdir(domain_path):\n",
    "\t\t\tfile_path = os.path.join(domain_path, filename)\n",
    "\t\t\ttry:\n",
    "\t\t\t\twith open(file_path, \"rb\") as f:\n",
    "\t\t\t\t\traw = f.read()\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tcontent = raw.decode(\"utf-8\").strip()\n",
    "\t\t\t\texcept UnicodeDecodeError:\n",
    "\t\t\t\t\tcontent = raw.decode(\"latin-1\").strip()\n",
    "\n",
    "\t\t\t\tfor line in content.splitlines():\n",
    "\t\t\t\t\tline = line.strip()\n",
    "\t\t\t\t\tif not line:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\tif \"#label#:\" in line:\n",
    "\t\t\t\t\t\ttext_part, label_part = line.split(\"#label#:\")\n",
    "\t\t\t\t\t\tlabel = label_part.strip()\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttext_part = line\n",
    "\t\t\t\t\t\tlabel = None\n",
    "\n",
    "\t\t\t\t\ttokens = [tok.split(\":\")[0] for tok in text_part.split()]\n",
    "\t\t\t\t\ttext = \" \".join(tokens)\n",
    "\n",
    "\t\t\t\t\tdata.append({\n",
    "\t\t\t\t\t\t\"document\": text,\n",
    "\t\t\t\t\t\t\"label\": label,\n",
    "\t\t\t\t\t})\n",
    "\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "\treturn pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a63af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_DATAFRAME = load_20_news(NEWS_DIR)\n",
    "MULTIDOMAIN_DATAFRAME = load_multidomain(MULTIDOMAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a8edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIDOMAIN_DATAFRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379ce6f7",
   "metadata": {},
   "source": [
    "# 3. Section marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81cc9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f5b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_20_news(text):\n",
    "    # Remove common email headers\n",
    "    text = re.sub(r'^(From|Subject|Lines|Organization|Reply-To|NNTP-Posting-Host|Keywords|Summary):.*$', '', text, flags = re.MULTILINE)\n",
    "\n",
    "    # Remove email addresses and URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    # Remove lines of signatures or separators\n",
    "    text = re.sub(r'--+\\s*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'_+', '', text)\n",
    "\n",
    "    # Remove quoted lines (beginning with > or :)\n",
    "    text = re.sub(r'(^>.*$|^:.*$)', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Collapse multiple newlines and spaces\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_multidomain(text):\n",
    "    # Replace underscores with spaces\n",
    "    text = text.replace(\"_\", \" \")\n",
    "\n",
    "    # Remove special tokens like <num>\n",
    "    text = re.sub(r\"<num>\", \"\", text)\n",
    "\n",
    "    # Remove multiple spaces\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_DATAFRAME[\"document_clean\"] = NEWS_DATAFRAME[\"document\"].apply(clean_20_news)\n",
    "MULTIDOMAIN_DATAFRAME[\"document_clean\"] = MULTIDOMAIN_DATAFRAME[\"document\"].apply(clean_multidomain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abf5013",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIDOMAIN_DATAFRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a1dc2",
   "metadata": {},
   "source": [
    "# 4. Section Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b2e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_news = LabelEncoder()\n",
    "label_encoder_news.fit(NEWS_DATAFRAME[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_DATAFRAME[\"label_id\"] = label_encoder_news.transform(NEWS_DATAFRAME[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc115833",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_multidomain = LabelEncoder()\n",
    "label_encoder_multidomain.fit(MULTIDOMAIN_DATAFRAME[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a06982",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIDOMAIN_DATAFRAME[\"label_id\"] = label_encoder_multidomain.transform(MULTIDOMAIN_DATAFRAME[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd43fbd",
   "metadata": {},
   "source": [
    "# 5. Section Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5764822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb79b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(dataframe):\n",
    "    \n",
    "\tX = dataframe[\"document_clean\"]\n",
    "\ty = dataframe[\"label_id\"]\n",
    "\n",
    "\t# 60% for training, 40% for second split\n",
    "\tX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size = 0.4, random_state = RANDOM_SEED, stratify = y)\n",
    "\n",
    "\t# 10% for validation, 30% for test\n",
    "\tX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size = 0.75, random_state = RANDOM_SEED, stratify = y_temp)\n",
    "\n",
    "\ttrain_df = X_train.to_frame(\"text\")\n",
    "\ttrain_df[\"label\"] = y_train.values\n",
    "\n",
    "\tval_df = X_val.to_frame(\"text\")\n",
    "\tval_df[\"label\"] = y_val.values\n",
    "\n",
    "\ttest_df = X_test.to_frame(\"text\")\n",
    "\ttest_df[\"label\"] = y_test.values\n",
    "\n",
    "\tprint(f\"Train size: {len(train_df)}\")\n",
    "\tprint(f\"Val size:   {len(val_df)}\")\n",
    "\tprint(f\"Test size:  {len(test_df)}\")\n",
    "\n",
    "\treturn train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e09b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_news, val_df_news, test_df_news = split_dataframe(NEWS_DATAFRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d13aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_multidomain, val_df_multidomain, test_df_multidomain = split_dataframe(MULTIDOMAIN_DATAFRAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4da873",
   "metadata": {},
   "source": [
    "# 6. Section Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de7fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base_uncased_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f5aa1a",
   "metadata": {},
   "source": [
    "# 7. Section Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "\tdef __init__(self, data, tokenizer):\n",
    "\t\tself.embeddings = tokenizer(data[\"text\"].values.tolist(), padding = 'max_length', truncation = True, max_length = 256, return_tensors = 'pt')\n",
    "\t\tself.labels = torch.tensor(data['label'].values).long()\n",
    "\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn {\n",
    "\t\t\t\"input_ids\": self.embeddings[\"input_ids\"][idx],\n",
    "\t\t\t\"token_type_ids\": self.embeddings[\"token_type_ids\"][idx],\n",
    "\t\t\t\"attention_mask\": self.embeddings[\"attention_mask\"][idx],\n",
    "\t\t\t\"labels\": self.labels[idx]\n",
    "    }\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4bc51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news_dataset = CustomDataset(train_df_news, bert_base_uncased_tokenizer)\n",
    "val_news_dataset = CustomDataset(val_df_news, bert_base_uncased_tokenizer)\n",
    "test_news_dataset = CustomDataset(test_df_news, bert_base_uncased_tokenizer)\n",
    "\n",
    "train_news_loader = DataLoader(train_news_dataset, batch_size = 32, shuffle = True)\n",
    "val_news_loader = DataLoader(val_news_dataset, batch_size = 32, shuffle = True)\n",
    "test_news_loader = DataLoader(test_news_dataset, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2473ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_multidomain_dataset = CustomDataset(train_df_multidomain, bert_base_uncased_tokenizer)\n",
    "val_multidomain_dataset = CustomDataset(val_df_multidomain, bert_base_uncased_tokenizer)\n",
    "test_multidomain_dataset = CustomDataset(test_df_multidomain, bert_base_uncased_tokenizer)\n",
    "\n",
    "train_multidomain_loader = DataLoader(train_multidomain_dataset, batch_size = 32, shuffle = True)\n",
    "val_multidomain_loader = DataLoader(val_multidomain_dataset, batch_size = 32, shuffle = True)\n",
    "test_multidomain_loader = DataLoader(test_multidomain_dataset, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e3b393",
   "metadata": {},
   "source": [
    "# 9. Section Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db883fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fdb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y, output, size):\n",
    "  y_pred = output.argmax(dim = -1).reshape(-1)\n",
    "  return (y.reshape(-1) == y_pred).sum().item() / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79647f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_function, optimizer):\n",
    "    \n",
    "\tloss = 0\n",
    "\taccuracy = 0\n",
    "\tbatch_num = 0\n",
    "\n",
    "\tmodel.train()\n",
    "\n",
    "\tfor batch in tqdm(dataloader):\n",
    "\t\tbatch_num += 1\n",
    "\t\tbatch = {key: v.to(device) for key, v in batch.items()}\n",
    "\t\toutput = model(**batch)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tbatch_loss = loss_function(output.logits, batch[\"labels\"])\n",
    "\t\tbatch_loss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\tloss += batch_loss.item()\n",
    "\n",
    "\t\taccuracy += get_accuracy(batch[\"labels\"], output.logits.detach(), len(dataloader.dataset))\n",
    "\n",
    "\tloss = loss / batch_num\n",
    "\tprint(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "\treturn loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6963a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, loss_function):\n",
    "\n",
    "  loss = 0\n",
    "  accuracy = 0\n",
    "  batch_num = 0\n",
    "\n",
    "  model.eval()\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "      batch_num += 1\n",
    "      batch = {key: v.to(device) for key, v in batch.items()}\n",
    "      output = model(**batch)\n",
    "      batch_loss = loss_function(output.logits, batch[\"labels\"])\n",
    "      loss += batch_loss.item()\n",
    "      accuracy += get_accuracy(batch[\"labels\"], output.logits.detach(), len(dataloader.dataset))\n",
    "\n",
    "  loss = loss / batch_num\n",
    "\n",
    "  print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "  return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1a26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, print_name, checkpoint_path = None, num_labels = None, model_name = None):\n",
    "\n",
    "\tif checkpoint_path is not None:\n",
    "\t\tif model_name and num_labels:\n",
    "\t\t\tmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = num_labels)\n",
    "\n",
    "\tstate_dict = torch.load(checkpoint_path, map_location=device)\n",
    "\tif any(k.startswith(\"_orig_mod.\") for k in state_dict.keys()):\n",
    "\t\tnew_state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n",
    "\t\tstate_dict = new_state_dict\n",
    "\n",
    "\tmodel.load_state_dict(state_dict)\n",
    "\tmodel.to(device)\n",
    "\tmodel.eval()\n",
    "\n",
    "\tall_preds, all_labels = [], []\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch in tqdm(dataloader, desc = \"Evaluating model\"):\n",
    "\t\t\tbatch = {k: v.to(device) for k, v in batch.items()}\n",
    "\t\t\toutputs = model(**batch)\n",
    "\t\t\tpreds = torch.argmax(outputs.logits, dim = -1)\n",
    "\t\t\tall_preds.extend(preds.cpu().numpy())\n",
    "\t\t\tall_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "\tacc = accuracy_score(all_labels, all_preds)\n",
    "\tprecision_micro = precision_score(all_labels, all_preds, average = 'micro')\n",
    "\trecall_micro = recall_score(all_labels, all_preds, average = 'micro')\n",
    "\tf1_micro = f1_score(all_labels, all_preds, average = 'micro')\n",
    "\n",
    "\tprecision_macro = precision_score(all_labels, all_preds, average = 'macro')\n",
    "\trecall_macro = recall_score(all_labels, all_preds, average = 'macro')\n",
    "\tf1_macro = f1_score(all_labels, all_preds, average = 'macro')\n",
    "\n",
    "\treport = classification_report(all_labels, all_preds, digits = 4)\n",
    "\n",
    "\tprint(f\"\\nFinal results {print_name} in test\")\n",
    "\tprint(f\"Accuracy: {acc:.4f}\")\n",
    "\tprint(f\"Precision (micro): {precision_micro:.4f}\")\n",
    "\tprint(f\"Recall (micro): {recall_micro:.4f}\")\n",
    "\tprint(f\"F1 (micro): {f1_micro:.4f}\")\n",
    "\tprint(f\"Precision (macro): {precision_macro:.4f}\")\n",
    "\tprint(f\"Recall (macro): {recall_macro:.4f}\")\n",
    "\tprint(f\"F1 (macro): {f1_macro:.4f}\")\n",
    "\tprint(\"\\nClassification Report:\")\n",
    "\tprint(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83930051",
   "metadata": {},
   "source": [
    "# 10. Section Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_20_news = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 20).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691aa45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_20_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61997bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_20_news = torch.compile(model_20_news.to(device), backend = \"eager\")\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model_20_news.parameters(), lr = 1e-5)\n",
    "model_20_news = model_20_news.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b962f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n  Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    train_loss, train_acc = train(train_news_loader, model_20_news, loss_function, optimizer)\n",
    "    val_loss, val_acc = evaluate(val_news_loader, model_20_news, loss_function)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model_20_news.state_dict(), \"../models/best_bert_base_uncased_model_news.pt\")\n",
    "        print(\"Model saved (best acc in validation so far)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model = None, \n",
    "               dataloader = test_news_loader, \n",
    "               device = device, \n",
    "               checkpoint_path = \"../models/best_bert_base_uncased_model_news.pt\", \n",
    "               num_labels = 20, \n",
    "               model_name = \"bert-base-uncased\", \n",
    "               print_name = \"BERT BASE UNCASED (20 NEWS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec83789",
   "metadata": {},
   "source": [
    "# 11. Section Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198efe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multidomain = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multidomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multidomain = torch.compile(model_multidomain.to(device), backend = \"eager\")\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model_multidomain.parameters(), lr = 1e-5)\n",
    "model_multidomain = model_multidomain.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9971b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n  Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    train_loss, train_acc = train(train_multidomain_loader, model_multidomain, loss_function, optimizer)\n",
    "    val_loss, val_acc = evaluate(val_multidomain_loader, model_multidomain, loss_function)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model_multidomain.state_dict(), \"../models/best_bert_base_uncased_model_multidomain.pt\")\n",
    "        print(\"Model saved (best acc in validation so far)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e6b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model = None, \n",
    "               dataloader = test_multidomain_loader, \n",
    "               device = device, \n",
    "               checkpoint_path = \"../models/best_bert_base_uncased_model_multidomain.pt\", \n",
    "               num_labels = 2, \n",
    "               model_name = \"bert-base-uncased\", \n",
    "               print_name = \"BERT BASE UNCASED (MULTIDOMAIN)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
